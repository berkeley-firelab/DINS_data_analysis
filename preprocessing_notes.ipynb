{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc2fdbd-2ed3-4ff4-b7c1-de4f9087182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# ML related imports\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "\n",
    "# AT utility imports\n",
    "from utils.directory_structure import DATA_DIR, OUTPUT_DIR\n",
    "from utils.preprocessing import read_split, pairwise_dist_imputer_catnum\n",
    "from utils.custom_encoder import custom_categorical_encoder\n",
    "\n",
    "\n",
    "mpl.style.use(\"bmh\")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faede306-9d3f-4104-bbce-b483e53dada1",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "Reading the `dins` original file, which is a heterogeneous tabular dataset. Then, conduct a simple data exploration on the distribution and nature of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89009dc-b267-4ca9-9258-6aaa51a76fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 69725 entries, 49272 to 49682\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   ROOFCONSTRUCTION   66280 non-null  object \n",
      " 1   EAVES              66220 non-null  object \n",
      " 2   VENTSCREEN         66180 non-null  object \n",
      " 3   EXTERIORSIDING     66267 non-null  object \n",
      " 4   WINDOWPANE         66212 non-null  object \n",
      " 5   DECKPORCHONGRADE   56244 non-null  object \n",
      " 6   DECKPORCHELEVATED  56242 non-null  object \n",
      " 7   PATIOCOVER         56238 non-null  object \n",
      " 8   FENCE              56242 non-null  object \n",
      " 9   YEARBUILT          42390 non-null  float64\n",
      " 10  LATITUDE           69725 non-null  float64\n",
      " 11  LONGITUDE          69725 non-null  float64\n",
      " 12  DISTANCE           69725 non-null  float64\n",
      " 13  utm_easting        69725 non-null  float64\n",
      " 14  utm_northing       69725 non-null  float64\n",
      " 15  utm_zone           69725 non-null  float64\n",
      "dtypes: float64(7), object(9)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "new_features = False\n",
    "case_name = \"dins_2017_2022\"\n",
    "\n",
    "fname = os.path.join(OUTPUT_DIR, f\"{case_name}_train_test_vars.pkl\")\n",
    "if new_features:\n",
    "    os.system(f\"rm {fname}\")\n",
    "    \n",
    "    X_train_full, X_test, y_train_full, y_test, col_names = read_split(f\"{case_name}.csv\")\n",
    "    data_to_save = {\"X_train_full\": X_train_full, \"X_test\": X_test, \n",
    "                    \"y_train_full\": y_train_full, \"y_test\": y_test}\n",
    "    \n",
    "    with open(fname, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "else:\n",
    "    with open(fname, 'rb') as file:\n",
    "        data_dict = pickle.load(file)\n",
    "    X_train_full = data_dict[\"X_train_full\"]\n",
    "    y_train_full = data_dict[\"y_train_full\"]\n",
    "    X_test = data_dict[\"X_test\"]\n",
    "    y_test = data_dict[\"y_test\"]\n",
    "\n",
    "X_train_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5082684-17bb-448c-b8fe-f9a6ce0c2b33",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31b6d6-bb5c-4ad7-894a-12203d0a32d0",
   "metadata": {},
   "source": [
    "The steps to data preprocessing is as follows:\n",
    "\n",
    "1. Separate the data into train and test cases with 20% going to the test set.\n",
    "2. Design imputation strategies, train and apply to the train set, and fit to the test set.\n",
    "3. To enable use of a variety of models:\n",
    "    - Normalize the numerical variables\n",
    "    - Conduct `OneHotEncoding` on categorical variables\n",
    "4. Resample to make the representation of all classes equal to in the train set.\n",
    "5. If necessary do a `PCA` conversion\n",
    "6. Put all steps into a pipelie under one function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f382f80-64e6-4633-927a-86f9b2cdd98a",
   "metadata": {},
   "source": [
    "## Imputation strategies\n",
    "\n",
    "The strategy differs for each type of `categorical` and `numerical` features and even within each category\n",
    "\n",
    "*DINS*: Adopted strategy for features with missing values in samples,\n",
    "\n",
    "- `ROOFCONSTRUCTION`  has `82817` non-null  objects: Nearest neighboor imputation.\n",
    "- `EAVES` has `82741` non-null  objects: Nearest neighboor imputation.\n",
    "- `VENTSCREEN` has `82692` non-null  objects: Nearest neighboor imputation.\n",
    "- `EXTERIORSIDING` has`82800` non-null  objects: Nearest neighboor imputation.\n",
    "- `WINDOWPANE` has `82732` non-null objects: Nearest neighboor imputation.\n",
    "- `DECKPORCHONGRADE` has `70291` non-null objects: Nearest neighboor imputation.\n",
    "- `DECKPORCHELEVATED` has `70290` non-null objects: Nearest neighboor imputation.\n",
    "- `PATIOCOVER` has `70286` non-null objects: Nearest neighboor imputation.\n",
    "- `FENCE` has `70289` non-null objects: Nearest neighboor imputation.\n",
    "- `YEARBUILT` has `53075` non-null objects: Nearest neighboor imputation.\n",
    "\n",
    "*Wildfire cases*: Adopted strategy for features with missing values in samples,\n",
    "\n",
    "- `ZIPCODE` has `15` non-null  floats: Reverse geoencoding can be used if this is useful. Potentially for future studies. \n",
    "- `ROOFCONSTR` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `EAVES` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `VENTSCREEN` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `EXTERIORSI` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `WINDOWPANE` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `DECKPORCHO` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `DECKPORCHE` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `PATIOCOVER` has `19318`  non-null samples: Nearest neighboor imputation\n",
    "- `FENCEATTAC` has `19317`  non-null samples: Nearest neighboor imputation\n",
    "- `YEARBUILT ` has `22501`  non-null samples: Nearest neighboor imputation or median\n",
    "- `VSD` has `3504 ` non-null  samples: Aggregate (mean, median, etc)\n",
    "- `EMBER` has `11549`  non-null samples: Aggregate (mean, median, etc) potentially with KNN\n",
    "- `FLAME` has `14578`  non-null samples: Aggregate (mean, median, etc) potentially with KNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e35d3-cc11-45cc-a862-845dff408775",
   "metadata": {},
   "source": [
    "# Impute data\n",
    "\n",
    "Imputation is done based on the statistics of the $k$ nearest neighbor points. For numerical values the aggregation is done using `mean` and for categorical the aggregation is done by `mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a14e59e-1629-4768-81d1-9a2214426c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = False\n",
    "fname = os.path.join(OUTPUT_DIR, \"dins_imputed_dataset.pkl\")\n",
    "\n",
    "if new_features:\n",
    "    X_train_nan_cols = X_train_full.columns[X_train_full.isna().any()].tolist()\n",
    "    X_train_full = pairwise_dist_imputer_catnum(X_train_full, nan_cols=X_train_nan_cols)\n",
    "    \n",
    "    X_test_nan_cols = X_test.columns[X_test.isna().any()].tolist()\n",
    "    X_test = pairwise_dist_imputer_catnum(X_test, nan_cols=X_test_nan_cols)\n",
    "    \n",
    "    data_dict = {\"X_train\": X_train_full, \"y_train\": y_train_full, \n",
    "                 \"X_test\": X_test, \"y_test\": y_test}\n",
    "    \n",
    "    with open(fname, 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "else:\n",
    "    with open(fname, 'rb') as file:\n",
    "        imputed_data = pickle.load(file)\n",
    "        \n",
    "    X_train = imputed_data[\"X_train\"]\n",
    "    y_train = imputed_data[\"y_train\"]\n",
    "    X_test  = imputed_data[\"X_test\"]\n",
    "    y_test  = imputed_data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ae2f-9ec4-47db-9e96-d69d50ced14f",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a862a90-5213-4e68-ad99-1a6c5ed740f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding features\n",
    "feature_encoder = custom_categorical_encoder()\n",
    "X_train_encoded = feature_encoder.fit_transform(X_train)\n",
    "X_test_encoded  = feature_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2df42-444c-4611-af7f-4399ec83c9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
